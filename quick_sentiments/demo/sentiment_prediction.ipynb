{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17dfb99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data 'punkt' already present.\n",
      "NLTK data 'stopwords' already present.\n",
      "Downloading NLTK data: wordnet...\n",
      "NLTK data 'wordnet' downloaded.\n",
      "Downloading NLTK data: omw-1.4...\n",
      "NLTK data 'omw-1.4' downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\meala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\meala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#load the packages\n",
    "import polars as pl\n",
    "#make sure that this is the main file\n",
    "import sys\n",
    "import os\n",
    "project_root = os.getcwd()\n",
    "sys.path.insert(0, project_root)\n",
    "# here I have three python script I built to pre_process the data and running the pipeline\n",
    "# you can find the code in the tools/preprocess.py file\n",
    "# you can find  the code in the tools/pipeline.py file\n",
    "# the pre_process function is used to clean the text data, there are various options available, please check the tools/preprocess.py file for details\n",
    "# the run_pipeline function is used to run the sentimental analysis pipeline, it takes the training data and the vectorizer and machine learning methods as input, and returns the results\n",
    "from tools.preprocess import pre_process\n",
    "#this function will run the sentimental analysis in the training data and return the results\n",
    "from tools.pipeline import run_pipeline\n",
    "# this function will run the sentimental analysis in the new data and return the predictions\n",
    "from tools.predict import make_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8273408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"Training Data/Train.csv\" #giive path to the training data\n",
    "df_train = pl.read_csv(path1, has_header=True, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "389c704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use the pre_process function to clean the text data\n",
    "response_column = \"reviewText\" # feel free to change the column name to your text column name\n",
    "sentiment_column = \"sentiment\" # feel free to change the column name to your label column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1bb9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meala\\AppData\\Local\\Temp\\ipykernel_17432\\3268764801.py:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df_train = df_train.with_columns(\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.with_columns(\n",
    "    pl.col(response_column).map_elements(lambda x: pre_process(x)).alias(\"processed\")  #add inside the map_elements\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24f707fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Pipeline for Bow + Logit ---\n",
      "WARNING: Dropped 6447 rows due to missing values (None) in 'processed' or 'sentiment' columns. Original rows: 162758, Rows after dropping: 156311\n",
      "Labels encoded: Original -> ['NEGATIVE' 'POSITIVE'], Encoded -> [0 1]\n",
      "1. Vectorizing entire dataset (X)...\n",
      "   - Generating Bag-of-Words features...\n",
      "2. Splitting data into train/test...\n",
      "3. Training and predicting...\n",
      "   - Training Logistic Regression with default parameters (no hyperparameter tuning)...\n",
      "   - Model trained with default parameters.\n",
      "Best model parameters: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "4. Evaluating model...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.74      0.63      0.68     10319\n",
      "    POSITIVE       0.83      0.89      0.86     20944\n",
      "\n",
      "    accuracy                           0.80     31263\n",
      "   macro avg       0.78      0.76      0.77     31263\n",
      "weighted avg       0.80      0.80      0.80     31263\n",
      "\n",
      "True labels distribution: Counter({1: 20944, 0: 10319})\n",
      "Predicted labels distribution: Counter({1: 22439, 0: 8824})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\meala\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "dt= run_pipeline(\n",
    "    vectorizer_name=\"BOW\", # BOW, tf, tfidf, wv\n",
    "    model_name=\"logit\", # logit, rf, XGB .#XGB takes long time, can not recommend using it on normal case\n",
    "    df=df_train,\n",
    "    text_column_name=\"processed\",  # this is the column name of the text data, \n",
    "    sentiment_column_name = \"sentiment\",\n",
    "    perform_tuning = False # make this true if you want to perform hyperparameter tuning, it will take longer time and \n",
    "                            # may run out of memory if the dataset is large,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e70b8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = \"New Data/Test.csv\" #give path to the test data\n",
    "df_test = pl.read_csv(path2, has_header=True, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd696ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meala\\AppData\\Local\\Temp\\ipykernel_14272\\1567300024.py:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  new_data = df_test.with_columns(\n"
     ]
    }
   ],
   "source": [
    "new_data = df_test.with_columns(\n",
    "    pl.col(response_column).map_elements(lambda x: pre_process(x, remove_brackets=True)).alias(\"processed\")  #add inside the map_elements\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f085ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_prediction= make_predictions(\n",
    "    new_data=new_data,\n",
    "    text_column_name=\"processed\",\n",
    "    vectorizer=dt[\"vectorizer_object\"],\n",
    "    best_model=dt[\"model_object\"],\n",
    "    label_encoder=dt[\"label_encoder\"],\n",
    "    prediction_column_name=\"sentiment_predictions\"  # Optional custom name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the predictions to a csv file\n",
    "# missing values in the text data will be removed\n",
    "sentiments_prediction.write_csv(\"New Data/sentiments_prediction.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
