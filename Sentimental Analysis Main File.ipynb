{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install the packages \n",
    "#%pip install nltk\n",
    "#%pip install polars\n",
    "#%pip install gensim\n",
    "#%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\meala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\meala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\meala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the packages\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "import polars as pl\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure that this is the main file\n",
    "import sys\n",
    "import os\n",
    "project_root = os.getcwd()\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset\n",
    "Please ensure that the training dataset is properly formatted and contains the necessary columns for sentiment analysis, such as \"Response\" for text data and \"Sentiment\" for labels. The dataset should be a located in Training Data folder and must be csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not change the code\n",
    "#keep you training dataset in the training data folder\n",
    "#this template uses csv files\n",
    "#please rename the columns to Response and Sentiment in the csv file. column names can be set in Python but this template does not automatically update the column names at any point\n",
    "\n",
    "df_train = pl.read_csv(\"Training Data\\Train.csv\",encoding='ISO-8859-1') \n",
    "#Replace the name \"Train.csv\" with your file name | alternatively, rename your file name as \"Train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Response</th><th>Sentiment</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;[NAME] I handicap Disable disa…</td><td>&quot;O&quot;</td></tr><tr><td>&quot;[NAME] I will clean the hospit…</td><td>&quot;O&quot;</td></tr><tr><td>&quot;1. There are no clear instruct…</td><td>&quot;N&quot;</td></tr><tr><td>&quot;2 times i left messages and ne…</td><td>&quot;O&quot;</td></tr><tr><td>&quot;A bit confusing st first,&nbsp;&nbsp;but…</td><td>&quot;P&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌─────────────────────────────────┬───────────┐\n",
       "│ Response                        ┆ Sentiment │\n",
       "│ ---                             ┆ ---       │\n",
       "│ str                             ┆ str       │\n",
       "╞═════════════════════════════════╪═══════════╡\n",
       "│ [NAME] I handicap Disable disa… ┆ O         │\n",
       "│ [NAME] I will clean the hospit… ┆ O         │\n",
       "│ 1. There are no clear instruct… ┆ N         │\n",
       "│ 2 times i left messages and ne… ┆ O         │\n",
       "│ A bit confusing st first,  but… ┆ P         │\n",
       "└─────────────────────────────────┴───────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tools.preprocess module loaded\n",
      "Functions available in module: ['WordNetLemmatizer', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'emoji', 'lemmatizer', 'numpy', 'pre_process', 're', 'remove_emojis', 'remove_extra_spaces', 'remove_html_tags', 'remove_numbers', 'remove_punctuation_from_token', 'remove_square_brackets', 'remove_urls_emails', 'simple_tokenizer', 'stop_words', 'stopwords', 'string', 'word_tokenize']\n"
     ]
    }
   ],
   "source": [
    "# here I have two python script I built to pre_process the data and running the pipeline\n",
    "# you can find the code in the tools/preprocess.py file\n",
    "# you can find  the code in the tools/pipeline.py file\n",
    "# the pre_process function is used to clean the text data, there are various options available, please check the tools/preprocess.py file for details\n",
    "# the run_pipeline function is used to run the sentimental analysis pipeline, it takes the training data and the vectorizer and machine learning methods as input, and returns the results\n",
    "import importlib\n",
    "from tools.preprocess import pre_process\n",
    "#this function will run the sentimental analysis in the training data and return the results\n",
    "from tools.pipeline import run_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"[NAME] I handicap Disable disabled Special I work Aha says it's a [ADDRESS] [PHONE NUMBER]\",\n",
       " 'name handicap disable disabled special work aha say address phone number',\n",
       " 'handicap disable disabled special work aha say')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"Response\"][0] , pre_process(df_train[\"Response\"][0]) , pre_process(df_train[\"Response\"][0],remove_brackets= True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meala\\AppData\\Local\\Temp\\ipykernel_3500\\559423697.py:3: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df_train = df_train.with_columns(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Response</th><th>Sentiment</th><th>processed</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;[NAME] I handicap Disable disa…</td><td>&quot;O&quot;</td><td>&quot;handicap disable disabled spec…</td></tr><tr><td>&quot;[NAME] I will clean the hospit…</td><td>&quot;O&quot;</td><td>&quot;clean hospital hide hospital s…</td></tr><tr><td>&quot;1. There are no clear instruct…</td><td>&quot;N&quot;</td><td>&quot;1 clear instruction extending …</td></tr><tr><td>&quot;2 times i left messages and ne…</td><td>&quot;O&quot;</td><td>&quot;2 time left message never rece…</td></tr><tr><td>&quot;A bit confusing st first,&nbsp;&nbsp;but…</td><td>&quot;P&quot;</td><td>&quot;bit confusing st first using t…</td></tr><tr><td>&quot;A excellent process that helps…</td><td>&quot;P&quot;</td><td>&quot;excellent process help navigat…</td></tr><tr><td>&quot;A little confused with checkin…</td><td>&quot;O&quot;</td><td>&quot;little confused checking thru …</td></tr><tr><td>&quot;A little hard to navigate&quot;</td><td>&quot;N&quot;</td><td>&quot;little hard navigate&quot;</td></tr><tr><td>&quot;A lot of the time the website …</td><td>&quot;N&quot;</td><td>&quot;lot time website website easy …</td></tr><tr><td>&quot;A surprisingly easy website to…</td><td>&quot;P&quot;</td><td>&quot;surprisingly easy website navi…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 3)\n",
       "┌─────────────────────────────────┬───────────┬─────────────────────────────────┐\n",
       "│ Response                        ┆ Sentiment ┆ processed                       │\n",
       "│ ---                             ┆ ---       ┆ ---                             │\n",
       "│ str                             ┆ str       ┆ str                             │\n",
       "╞═════════════════════════════════╪═══════════╪═════════════════════════════════╡\n",
       "│ [NAME] I handicap Disable disa… ┆ O         ┆ handicap disable disabled spec… │\n",
       "│ [NAME] I will clean the hospit… ┆ O         ┆ clean hospital hide hospital s… │\n",
       "│ 1. There are no clear instruct… ┆ N         ┆ 1 clear instruction extending … │\n",
       "│ 2 times i left messages and ne… ┆ O         ┆ 2 time left message never rece… │\n",
       "│ A bit confusing st first,  but… ┆ P         ┆ bit confusing st first using t… │\n",
       "│ A excellent process that helps… ┆ P         ┆ excellent process help navigat… │\n",
       "│ A little confused with checkin… ┆ O         ┆ little confused checking thru … │\n",
       "│ A little hard to navigate       ┆ N         ┆ little hard navigate            │\n",
       "│ A lot of the time the website … ┆ N         ┆ lot time website website easy … │\n",
       "│ A surprisingly easy website to… ┆ P         ┆ surprisingly easy website navi… │\n",
       "└─────────────────────────────────┴───────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make changes as necessary\n",
    "# inside the map_elements, add  the parameters [pre_process(x, parameters_to_be_added)] and set it True/False if it differs from the defualt value\n",
    "df_train = df_train.with_columns(\n",
    "    pl.col(\"Response\").map_elements(lambda x: pre_process(x, remove_brackets=True)).alias(\"processed\")  #add inside the map_elements\n",
    ")\n",
    "\n",
    "df_train.head(10)\n",
    "#output might show warnings, it usually is not any problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### in this template, there are four text representation / vectorizer methods available \n",
    "#### #in the function run_pipeline, we shall make use of this, write the words inside [ ] for the methods you want to use\n",
    "#### 1. Bag of Words [BOW] \n",
    "#### 2. Term Frequency [tf]\n",
    "#### 3. TF -IDF    [tfidf]\n",
    "#### 4. Word Embedding using Word2Vec (you can use other packages with slight changes) [wv] \n",
    "         # Word Embedding uses defualt 300 values; this will take some time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### in this template, there are also three machine learning methods that can be used\n",
    "#### 1. Logistic Regression [logit]\n",
    "#### 2. Random forest (recommended) (rf)\n",
    "#### 3. XGBoosting  [XGB](word embedding and XGBoost may take long time to complete, combination of both is not recommended in local machine)\n",
    "\n",
    "#I will keep this repository updated, and I will add more methods in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained word2vec-google-news-300 model (this may take a few minutes)...\n",
      "Word2Vec model loaded.\n",
      "   - Starting Logistic Regression training with GridSearchCV for hyperparameter tuning...\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\meala\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1288: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   - Best Hyperparameters found:\n",
      "{'C': 10.0, 'class_weight': 'balanced', 'max_iter': 500, 'solver': 'liblinear'}\n",
      "   - Best Cross-Validation Score (F1-weighted): 0.7485\n",
      "Best model parameters: {'C': 10.0, 'class_weight': 'balanced', 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 500, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87       176\n",
      "           1       0.53      0.34      0.42        47\n",
      "           2       0.82      0.88      0.85        69\n",
      "\n",
      "    accuracy                           0.81       292\n",
      "   macro avg       0.73      0.71      0.71       292\n",
      "weighted avg       0.79      0.81      0.80       292\n",
      "\n",
      "Labels encoded: Original -> ['N' 'O' 'P'], Encoded -> [0 1 2]\n",
      "True labels distribution: Counter({0: 176, 2: 69, 1: 47})\n",
      "Predicted labels distribution: Counter({0: 188, 2: 74, 1: 30})\n"
     ]
    }
   ],
   "source": [
    "#this is the example of how to use the function\n",
    "#you can change the vectorizer_name and model_name to the ones you want to use\n",
    "# for now we will use word embedding and logistic regression\n",
    "\n",
    "# run_pipeline function will return the dataframe with the vectorized text, vectorizer used  and the model\n",
    "#it will also print the results of the model, including the accuracy and F1 score\n",
    "dt,vect,ml_model = run_pipeline(\n",
    "    vectorizer_name=\"wv\", # BOW, tf, tfidf, wv\n",
    "    model_name=\"logit\", # logit, rf, XGB .#XGB takes long time, can not recommend using it on normal case\n",
    "    df=df_train\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrame constructor called with unsupported type 'csr_matrix' for the `data` parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3500\\3118938117.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#View the dataframe after vectorization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#does not work when vectorizer_name is BOW\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#use the snippet below line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\meala\\anaconda3\\Lib\\site-packages\\polars\\dataframe\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, data, schema, schema_overrides, strict, orient, infer_schema_length, nan_to_null)\u001b[0m\n\u001b[0;32m    456\u001b[0m             msg = (\n\u001b[0;32m    457\u001b[0m                 \u001b[1;34mf\"DataFrame constructor called with unsupported type {type(data).__name__!r}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                 \u001b[1;34m\" for the `data` parameter\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m             )\n\u001b[1;32m--> 460\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: DataFrame constructor called with unsupported type 'csr_matrix' for the `data` parameter"
     ]
    }
   ],
   "source": [
    "#View the dataframe after vectorization\n",
    "pl.DataFrame(dt).head(10) #does not work when vectorizer_name is BOW \n",
    "#use the snippet below line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_459, 3_469)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column_0</th><th>column_1</th><th>column_2</th><th>column_3</th><th>column_4</th><th>column_5</th><th>column_6</th><th>column_7</th><th>column_8</th><th>column_9</th><th>column_10</th><th>column_11</th><th>column_12</th><th>column_13</th><th>column_14</th><th>column_15</th><th>column_16</th><th>column_17</th><th>column_18</th><th>column_19</th><th>column_20</th><th>column_21</th><th>column_22</th><th>column_23</th><th>column_24</th><th>column_25</th><th>column_26</th><th>column_27</th><th>column_28</th><th>column_29</th><th>column_30</th><th>column_31</th><th>column_32</th><th>column_33</th><th>column_34</th><th>column_35</th><th>column_36</th><th>&hellip;</th><th>column_3432</th><th>column_3433</th><th>column_3434</th><th>column_3435</th><th>column_3436</th><th>column_3437</th><th>column_3438</th><th>column_3439</th><th>column_3440</th><th>column_3441</th><th>column_3442</th><th>column_3443</th><th>column_3444</th><th>column_3445</th><th>column_3446</th><th>column_3447</th><th>column_3448</th><th>column_3449</th><th>column_3450</th><th>column_3451</th><th>column_3452</th><th>column_3453</th><th>column_3454</th><th>column_3455</th><th>column_3456</th><th>column_3457</th><th>column_3458</th><th>column_3459</th><th>column_3460</th><th>column_3461</th><th>column_3462</th><th>column_3463</th><th>column_3464</th><th>column_3465</th><th>column_3466</th><th>column_3467</th><th>column_3468</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>&hellip;</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1_459, 3_469)\n",
       "┌──────────┬──────────┬──────────┬──────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
       "│ column_0 ┆ column_1 ┆ column_2 ┆ column_3 ┆ … ┆ column_346 ┆ column_346 ┆ column_346 ┆ column_34 │\n",
       "│ ---      ┆ ---      ┆ ---      ┆ ---      ┆   ┆ 5          ┆ 6          ┆ 7          ┆ 68        │\n",
       "│ i64      ┆ i64      ┆ i64      ┆ i64      ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---       │\n",
       "│          ┆          ┆          ┆          ┆   ┆ i64        ┆ i64        ┆ i64        ┆ i64       │\n",
       "╞══════════╪══════════╪══════════╪══════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
       "│ 0        ┆ 0        ┆ 0        ┆ 0        ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 0         │\n",
       "│ 0        ┆ 0        ┆ 0        ┆ 0        ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 0         │\n",
       "│ 0        ┆ 0        ┆ 0        ┆ 0        ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 0         │\n",
       "│ 0        ┆ 0        ┆ 0        ┆ 0        ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 0         │\n",
       "│ 0        ┆ 0        ┆ 0        ┆ 0        ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 0         │\n",
       "│ …        ┆ …        ┆ …        ┆ …        ┆ … ┆ …          ┆ …          ┆ …          ┆ …         │\n",
       "│ 0        ┆ 0        ┆ 0        ┆ 0        ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 0         │\n",
       "│ 0        ┆ 0        ┆ 0        ┆ 0        ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 0         │\n",
       "│ 0        ┆ 0        ┆ 0        ┆ 0        ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 0         │\n",
       "│ 0        ┆ 0        ┆ 0        ┆ 0        ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 0         │\n",
       "│ 0        ┆ 0        ┆ 0        ┆ 0        ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 0         │\n",
       "└──────────┴──────────┴──────────┴──────────┴───┴────────────┴────────────┴────────────┴───────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#works only for BOW\n",
    "import numpy\n",
    "dt_view = dt.toarray()\n",
    "pl.DataFrame(dt_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Dataset for prediction\n",
    "You can use the same format as the training dataset, but ensure that it contains the \"Response\" column for text data. The \"Sentiment\" column is optional for prediction datasets, as it will be generated by the model.\n",
    "Make sure the dataset is saved in the \"New Data\" folder and is in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pl.read_csv(\"New Data/test_142.csv\",encoding='ISO-8859-1') #keep your file here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meala\\AppData\\Local\\Temp\\ipykernel_3500\\3171630237.py:6: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  new_data = new_data.with_columns(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Response</th><th>Sentiment</th><th>processed</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;[EMPLOYEE&#x27;S NAME] was great, v…</td><td>&quot;P&quot;</td><td>&quot;great personable helped unders…</td></tr><tr><td>&quot;[EMPLOYEE&#x27;S NAME] was so very …</td><td>&quot;P&quot;</td><td>&quot;helpful called back twice went…</td></tr><tr><td>&quot;[EMPLOYEE&#x27;S NAME] was very hel…</td><td>&quot;P&quot;</td><td>&quot;helpful nice came visit&quot;</td></tr><tr><td>&quot;2 claims were filed because yo…</td><td>&quot;N&quot;</td><td>&quot;2 claim filed employee gave wr…</td></tr><tr><td>&quot;2 of 3 elevators were out of o…</td><td>&quot;N&quot;</td><td>&quot;2 3 elevator order edd 6th flo…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────────────────────────────┬───────────┬─────────────────────────────────┐\n",
       "│ Response                        ┆ Sentiment ┆ processed                       │\n",
       "│ ---                             ┆ ---       ┆ ---                             │\n",
       "│ str                             ┆ str       ┆ str                             │\n",
       "╞═════════════════════════════════╪═══════════╪═════════════════════════════════╡\n",
       "│ [EMPLOYEE'S NAME] was great, v… ┆ P         ┆ great personable helped unders… │\n",
       "│ [EMPLOYEE'S NAME] was so very … ┆ P         ┆ helpful called back twice went… │\n",
       "│ [EMPLOYEE'S NAME] was very hel… ┆ P         ┆ helpful nice came visit         │\n",
       "│ 2 claims were filed because yo… ┆ N         ┆ 2 claim filed employee gave wr… │\n",
       "│ 2 of 3 elevators were out of o… ┆ N         ┆ 2 3 elevator order edd 6th flo… │\n",
       "└─────────────────────────────────┴───────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to ensure the new data has the correct column name\n",
    "# Can be changed on map_elements - pre_process() as you want\n",
    "# Do not change anything else\n",
    "first_col = new_data.columns[0]\n",
    "new_data = new_data.rename({first_col: \"Response\"})\n",
    "new_data = new_data.with_columns(\n",
    "    pl.col(\"Response\").map_elements(lambda x: pre_process(x, remove_brackets=True)).alias('processed') # processed text column like before\n",
    "    #make sure to use the same pre_process function as before, with the same parameters (remove_brackets=True in this case\n",
    ")\n",
    "new_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pipeline(df, vectorizer_func, ml_model):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of new data using the provided vectorizer and model functions.\n",
    "    \n",
    "    Parameters:\n",
    "    - new_data: DataFrame containing the new data to predict.\n",
    "    - vectorizer_func: Function to vectorize the text data.\n",
    "    - model_func: Function to apply the trained model for prediction.\n",
    "    \n",
    "    Returns:\n",
    "    - predictions: Array of predicted sentiments.\n",
    "    \"\"\"\n",
    "\n",
    "        # Prepare data\n",
    "    X_text = df[\"processed\"].to_list()\n",
    "    y_raw = df[\"Sentiment\"].to_list()\n",
    "\n",
    "    #XGBoost need Label Encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y_raw)\n",
    "    \n",
    "    # Vectorize the new data\n",
    "    X_new = vectorizer_func(X_text)  # Passes the processed text to the vectorizer function, which returns the vectorized representation\n",
    "    \n",
    "\n",
    "    # This is new data, so we do not split it into train and test sets\n",
    "    # Instead, we directly use the vectorized data for prediction\n",
    "    # Note, we are not training the model here, and these data should not be used for training to avoid data leakage\n",
    "    \n",
    "    # Predict using the model\n",
    "    predictions = ml_model.predict(X_new)\n",
    "    new_predictions_original_labels = label_encoder.inverse_transform(predictions)\n",
    "    \n",
    "    return  new_predictions_original_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using already loaded Word2Vec model.\n",
      "Using already loaded Word2Vec model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Response</th><th>Sentiment</th><th>processed</th><th>predictions</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;[EMPLOYEE&#x27;S NAME] was great, v…</td><td>&quot;P&quot;</td><td>&quot;great personable helped unders…</td><td>&quot;P&quot;</td></tr><tr><td>&quot;[EMPLOYEE&#x27;S NAME] was so very …</td><td>&quot;P&quot;</td><td>&quot;helpful called back twice went…</td><td>&quot;P&quot;</td></tr><tr><td>&quot;[EMPLOYEE&#x27;S NAME] was very hel…</td><td>&quot;P&quot;</td><td>&quot;helpful nice came visit&quot;</td><td>&quot;P&quot;</td></tr><tr><td>&quot;2 claims were filed because yo…</td><td>&quot;N&quot;</td><td>&quot;2 claim filed employee gave wr…</td><td>&quot;N&quot;</td></tr><tr><td>&quot;2 of 3 elevators were out of o…</td><td>&quot;N&quot;</td><td>&quot;2 3 elevator order edd 6th flo…</td><td>&quot;N&quot;</td></tr><tr><td>&quot;A pitiful, shame that in order…</td><td>&quot;N&quot;</td><td>&quot;pitiful shame order forward mo…</td><td>&quot;N&quot;</td></tr><tr><td>&quot;Almost impossible to get an ap…</td><td>&quot;N&quot;</td><td>&quot;almost impossible get appointm…</td><td>&quot;N&quot;</td></tr><tr><td>&quot;Asian representative was not c…</td><td>&quot;N&quot;</td><td>&quot;asian representative cooperati…</td><td>&quot;N&quot;</td></tr><tr><td>&quot;Cannot reach anyone through th…</td><td>&quot;N&quot;</td><td>&quot;reach anyone phone system plea…</td><td>&quot;N&quot;</td></tr><tr><td>&quot;clients are taking time away f…</td><td>&quot;N&quot;</td><td>&quot;client taking time away care c…</td><td>&quot;N&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 4)\n",
       "┌─────────────────────────────────┬───────────┬─────────────────────────────────┬─────────────┐\n",
       "│ Response                        ┆ Sentiment ┆ processed                       ┆ predictions │\n",
       "│ ---                             ┆ ---       ┆ ---                             ┆ ---         │\n",
       "│ str                             ┆ str       ┆ str                             ┆ str         │\n",
       "╞═════════════════════════════════╪═══════════╪═════════════════════════════════╪═════════════╡\n",
       "│ [EMPLOYEE'S NAME] was great, v… ┆ P         ┆ great personable helped unders… ┆ P           │\n",
       "│ [EMPLOYEE'S NAME] was so very … ┆ P         ┆ helpful called back twice went… ┆ P           │\n",
       "│ [EMPLOYEE'S NAME] was very hel… ┆ P         ┆ helpful nice came visit         ┆ P           │\n",
       "│ 2 claims were filed because yo… ┆ N         ┆ 2 claim filed employee gave wr… ┆ N           │\n",
       "│ 2 of 3 elevators were out of o… ┆ N         ┆ 2 3 elevator order edd 6th flo… ┆ N           │\n",
       "│ A pitiful, shame that in order… ┆ N         ┆ pitiful shame order forward mo… ┆ N           │\n",
       "│ Almost impossible to get an ap… ┆ N         ┆ almost impossible get appointm… ┆ N           │\n",
       "│ Asian representative was not c… ┆ N         ┆ asian representative cooperati… ┆ N           │\n",
       "│ Cannot reach anyone through th… ┆ N         ┆ reach anyone phone system plea… ┆ N           │\n",
       "│ clients are taking time away f… ┆ N         ┆ client taking time away care c… ┆ N           │\n",
       "└─────────────────────────────────┴───────────┴─────────────────────────────────┴─────────────┘"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THE CODE BELOW\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "predict_pipeline(new_data, vect, ml_model)\n",
    "new_data = new_data.with_columns(\n",
    "    pl.Series(name=\"predictions\", values=predict_pipeline(new_data, vect, ml_model))\n",
    ")\n",
    "\n",
    "new_data.head(10)  # Display the first 10 rows of the DataFrame with predictions\n",
    "# Add predictions to the DataFrame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
