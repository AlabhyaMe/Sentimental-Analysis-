{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the packages\n",
    "import polars as pl\n",
    "#make sure that this is the main file\n",
    "import sys\n",
    "import os\n",
    "project_root = os.getcwd()\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: 162758 rows and 5 columns\n"
     ]
    }
   ],
   "source": [
    "# keep you training dataset in the training data folder\n",
    "# this template uses csv files \n",
    "# column names can be set in Python but this template does not automatically update the column for the demo \n",
    "# however, the function will give you the option to tell column names for the text and label data\n",
    "\n",
    "df_train = pl.read_csv(\"Training Data/Train.csv\",encoding='ISO-8859-1') \n",
    "print(f\"Dataset shape: {df_train.shape[0]} rows and {df_train.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (40_689, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>movieid</th><th>reviewerName</th><th>isFrequentReviewer</th><th>reviewText</th><th>sentiment</th></tr><tr><td>str</td><td>str</td><td>bool</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;john_mcclane_rocky_balboa_laby…</td><td>&quot;Deborah Farley&quot;</td><td>false</td><td>&quot;..a cocksure, stylized, gutty …</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;tyler_durden_dazzling_incredib…</td><td>&quot;Margaret Martin&quot;</td><td>false</td><td>&quot;A wild ride of a movie, Spring…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;glimmer_mythical_han_solo&quot;</td><td>&quot;Angel Peters&quot;</td><td>false</td><td>&quot;In a year of many car movies, …</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;gandalf_journey_epic&quot;</td><td>&quot;Alexandria Wilson&quot;</td><td>true</td><td>&quot;Festival will surely tickle yo…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;vito_corleone_lara_croft_travi…</td><td>&quot;Andrew Blankenship&quot;</td><td>false</td><td>&quot;The limp&amp;#44; negligible resul…</td><td>&quot;NEGATIVE&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;rick_blaine_indiana_jones_hidd…</td><td>&quot;Stephanie Medina DDS&quot;</td><td>false</td><td>&quot;I think overall The Upside is …</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;hulk_scarlett_o&#x27;hara_don_vito_…</td><td>&quot;Katrina Briggs&quot;</td><td>true</td><td>&quot;With Arrival, Villeneuve final…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;wolverine_the_terminator_darth…</td><td>&quot;Ashley Santos&quot;</td><td>false</td><td>&quot;It&#x27;s not enough to copy your f…</td><td>&quot;NEGATIVE&quot;</td></tr><tr><td>&quot;james_bond_the_glorious&quot;</td><td>&quot;Sharon Foster&quot;</td><td>false</td><td>&quot;A conventional and entirely di…</td><td>&quot;NEGATIVE&quot;</td></tr><tr><td>&quot;edward_scissorhands_tony_monta…</td><td>&quot;Sherry Vega&quot;</td><td>false</td><td>&quot;There&#x27;s not a single surprise …</td><td>&quot;NEGATIVE&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (40_689, 5)\n",
       "┌─────────────────────┬─────────────────────┬────────────────────┬─────────────────────┬───────────┐\n",
       "│ movieid             ┆ reviewerName        ┆ isFrequentReviewer ┆ reviewText          ┆ sentiment │\n",
       "│ ---                 ┆ ---                 ┆ ---                ┆ ---                 ┆ ---       │\n",
       "│ str                 ┆ str                 ┆ bool               ┆ str                 ┆ str       │\n",
       "╞═════════════════════╪═════════════════════╪════════════════════╪═════════════════════╪═══════════╡\n",
       "│ john_mcclane_rocky_ ┆ Deborah Farley      ┆ false              ┆ ..a cocksure,       ┆ POSITIVE  │\n",
       "│ balboa_laby…        ┆                     ┆                    ┆ stylized, gutty …   ┆           │\n",
       "│ tyler_durden_dazzli ┆ Margaret Martin     ┆ false              ┆ A wild ride of a    ┆ POSITIVE  │\n",
       "│ ng_incredib…        ┆                     ┆                    ┆ movie, Spring…      ┆           │\n",
       "│ glimmer_mythical_ha ┆ Angel Peters        ┆ false              ┆ In a year of many   ┆ POSITIVE  │\n",
       "│ n_solo              ┆                     ┆                    ┆ car movies, …       ┆           │\n",
       "│ gandalf_journey_epi ┆ Alexandria Wilson   ┆ true               ┆ Festival will       ┆ POSITIVE  │\n",
       "│ c                   ┆                     ┆                    ┆ surely tickle yo…   ┆           │\n",
       "│ vito_corleone_lara_ ┆ Andrew Blankenship  ┆ false              ┆ The limp&#44;       ┆ NEGATIVE  │\n",
       "│ croft_travi…        ┆                     ┆                    ┆ negligible resul…   ┆           │\n",
       "│ …                   ┆ …                   ┆ …                  ┆ …                   ┆ …         │\n",
       "│ rick_blaine_indiana ┆ Stephanie Medina    ┆ false              ┆ I think overall The ┆ POSITIVE  │\n",
       "│ _jones_hidd…        ┆ DDS                 ┆                    ┆ Upside is …         ┆           │\n",
       "│ hulk_scarlett_o'har ┆ Katrina Briggs      ┆ true               ┆ With Arrival,       ┆ POSITIVE  │\n",
       "│ a_don_vito_…        ┆                     ┆                    ┆ Villeneuve final…   ┆           │\n",
       "│ wolverine_the_termi ┆ Ashley Santos       ┆ false              ┆ It's not enough to  ┆ NEGATIVE  │\n",
       "│ nator_darth…        ┆                     ┆                    ┆ copy your f…        ┆           │\n",
       "│ james_bond_the_glor ┆ Sharon Foster       ┆ false              ┆ A conventional and  ┆ NEGATIVE  │\n",
       "│ ious                ┆                     ┆                    ┆ entirely di…        ┆           │\n",
       "│ edward_scissorhands ┆ Sherry Vega         ┆ false              ┆ There's not a       ┆ NEGATIVE  │\n",
       "│ _tony_monta…        ┆                     ┆                    ┆ single surprise …   ┆           │\n",
       "└─────────────────────┴─────────────────────┴────────────────────┴─────────────────────┴───────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()\n",
    "# randomly select only 25% of the data since the dataset is large\n",
    "\n",
    "df_train = df_train.sample(fraction=0.25, shuffle=True, seed=42)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data 'punkt' already present.\n",
      "NLTK data 'stopwords' already present.\n",
      "Downloading NLTK data: wordnet...\n",
      "NLTK data 'wordnet' downloaded.\n",
      "Downloading NLTK data: omw-1.4...\n",
      "NLTK data 'omw-1.4' downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\meala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\meala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# here I have three python script I built to pre_process the data and running the pipeline\n",
    "# you can find the code in the tools/preprocess.py file\n",
    "# you can find  the code in the tools/pipeline.py file\n",
    "# the pre_process function is used to clean the text data, there are various options available, please check the tools/preprocess.py file for details\n",
    "# the run_pipeline function is used to run the sentimental analysis pipeline, it takes the training data and the vectorizer and machine learning methods as input, and returns the results\n",
    "from tools.preprocess import pre_process\n",
    "#this function will run the sentimental analysis in the training data and return the results\n",
    "from tools.pipeline import run_pipeline\n",
    "# this function will run the sentimental analysis in the new data and return the predictions\n",
    "from tools.predict import make_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a year of many car movies, this one was the best (Living Life Fearless)\n",
      "\n",
      "year many car movie one best living life fearless\n"
     ]
    }
   ],
   "source": [
    "# you can use the pre_process function to clean the text data\n",
    "response_column = \"reviewText\" # feel free to change the column name to your text column name\n",
    "sentiment_column = \"sentiment\" # feel free to change the column name to your label column name\n",
    "print(df_train[response_column][2])\n",
    "print(\"\\n\" + pre_process(df_train[response_column][2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meala\\AppData\\Local\\Temp\\ipykernel_17248\\2369074634.py:3: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df_train = df_train.with_columns(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>movieid</th><th>reviewerName</th><th>isFrequentReviewer</th><th>reviewText</th><th>sentiment</th><th>processed</th></tr><tr><td>str</td><td>str</td><td>bool</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;john_mcclane_rocky_balboa_laby…</td><td>&quot;Deborah Farley&quot;</td><td>false</td><td>&quot;..a cocksure, stylized, gutty …</td><td>&quot;POSITIVE&quot;</td><td>&quot;cocksure stylized gutty thrill…</td></tr><tr><td>&quot;tyler_durden_dazzling_incredib…</td><td>&quot;Margaret Martin&quot;</td><td>false</td><td>&quot;A wild ride of a movie, Spring…</td><td>&quot;POSITIVE&quot;</td><td>&quot;wild ride movie spring breaker…</td></tr><tr><td>&quot;glimmer_mythical_han_solo&quot;</td><td>&quot;Angel Peters&quot;</td><td>false</td><td>&quot;In a year of many car movies, …</td><td>&quot;POSITIVE&quot;</td><td>&quot;year many car movie one best l…</td></tr><tr><td>&quot;gandalf_journey_epic&quot;</td><td>&quot;Alexandria Wilson&quot;</td><td>true</td><td>&quot;Festival will surely tickle yo…</td><td>&quot;POSITIVE&quot;</td><td>&quot;festival surely tickle funny b…</td></tr><tr><td>&quot;vito_corleone_lara_croft_travi…</td><td>&quot;Andrew Blankenship&quot;</td><td>false</td><td>&quot;The limp&amp;#44; negligible resul…</td><td>&quot;NEGATIVE&quot;</td><td>&quot;limp 44 negligible result unwe…</td></tr><tr><td>&quot;zephyr_scarlett_o&#x27;hara_magicia…</td><td>&quot;Lee Griffin&quot;</td><td>false</td><td>&quot;The film does have it&#x27;s weak s…</td><td>&quot;POSITIVE&quot;</td><td>&quot;film weak spot part dead see&quot;</td></tr><tr><td>&quot;the_joker_quest_rocky_balboa&quot;</td><td>&quot;Brianna Flores&quot;</td><td>true</td><td>&quot;A good-hearted entertainment t…</td><td>&quot;POSITIVE&quot;</td><td>&quot;goodhearted entertainment mana…</td></tr><tr><td>&quot;surreal_the_captain_america_go…</td><td>&quot;Shelia Miller&quot;</td><td>false</td><td>&quot;At this moment, 500 Days of Su…</td><td>&quot;POSITIVE&quot;</td><td>&quot;moment 500 day summer favorite…</td></tr><tr><td>&quot;dorothy_gale_gollum_frodo_bagg…</td><td>&quot;Chelsea Martinez&quot;</td><td>false</td><td>&quot;Beats thrums with an unbridled…</td><td>&quot;POSITIVE&quot;</td><td>&quot;beat thrum unbridled energy ow…</td></tr><tr><td>&quot;phenomenal_valiant_michael_cor…</td><td>&quot;Mike Joseph&quot;</td><td>false</td><td>&quot;A sequel that - narratively an…</td><td>&quot;POSITIVE&quot;</td><td>&quot;sequel narratively visually se…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 6)\n",
       "┌─────────────────┬─────────────────┬────────────────┬────────────────┬───────────┬────────────────┐\n",
       "│ movieid         ┆ reviewerName    ┆ isFrequentRevi ┆ reviewText     ┆ sentiment ┆ processed      │\n",
       "│ ---             ┆ ---             ┆ ewer           ┆ ---            ┆ ---       ┆ ---            │\n",
       "│ str             ┆ str             ┆ ---            ┆ str            ┆ str       ┆ str            │\n",
       "│                 ┆                 ┆ bool           ┆                ┆           ┆                │\n",
       "╞═════════════════╪═════════════════╪════════════════╪════════════════╪═══════════╪════════════════╡\n",
       "│ john_mcclane_ro ┆ Deborah Farley  ┆ false          ┆ ..a cocksure,  ┆ POSITIVE  ┆ cocksure       │\n",
       "│ cky_balboa_laby ┆                 ┆                ┆ stylized,      ┆           ┆ stylized gutty │\n",
       "│ …               ┆                 ┆                ┆ gutty …        ┆           ┆ thrill…        │\n",
       "│ tyler_durden_da ┆ Margaret Martin ┆ false          ┆ A wild ride of ┆ POSITIVE  ┆ wild ride      │\n",
       "│ zzling_incredib ┆                 ┆                ┆ a movie,       ┆           ┆ movie spring   │\n",
       "│ …               ┆                 ┆                ┆ Spring…        ┆           ┆ breaker…       │\n",
       "│ glimmer_mythica ┆ Angel Peters    ┆ false          ┆ In a year of   ┆ POSITIVE  ┆ year many car  │\n",
       "│ l_han_solo      ┆                 ┆                ┆ many car       ┆           ┆ movie one best │\n",
       "│                 ┆                 ┆                ┆ movies, …      ┆           ┆ l…             │\n",
       "│ gandalf_journey ┆ Alexandria      ┆ true           ┆ Festival will  ┆ POSITIVE  ┆ festival       │\n",
       "│ _epic           ┆ Wilson          ┆                ┆ surely tickle  ┆           ┆ surely tickle  │\n",
       "│                 ┆                 ┆                ┆ yo…            ┆           ┆ funny b…       │\n",
       "│ vito_corleone_l ┆ Andrew          ┆ false          ┆ The limp&#44;  ┆ NEGATIVE  ┆ limp 44        │\n",
       "│ ara_croft_travi ┆ Blankenship     ┆                ┆ negligible     ┆           ┆ negligible     │\n",
       "│ …               ┆                 ┆                ┆ resul…         ┆           ┆ result unwe…   │\n",
       "│ zephyr_scarlett ┆ Lee Griffin     ┆ false          ┆ The film does  ┆ POSITIVE  ┆ film weak spot │\n",
       "│ _o'hara_magicia ┆                 ┆                ┆ have it's weak ┆           ┆ part dead see  │\n",
       "│ …               ┆                 ┆                ┆ s…             ┆           ┆                │\n",
       "│ the_joker_quest ┆ Brianna Flores  ┆ true           ┆ A good-hearted ┆ POSITIVE  ┆ goodhearted    │\n",
       "│ _rocky_balboa   ┆                 ┆                ┆ entertainment  ┆           ┆ entertainment  │\n",
       "│                 ┆                 ┆                ┆ t…             ┆           ┆ mana…          │\n",
       "│ surreal_the_cap ┆ Shelia Miller   ┆ false          ┆ At this        ┆ POSITIVE  ┆ moment 500 day │\n",
       "│ tain_america_go ┆                 ┆                ┆ moment, 500    ┆           ┆ summer         │\n",
       "│ …               ┆                 ┆                ┆ Days of Su…    ┆           ┆ favorite…      │\n",
       "│ dorothy_gale_go ┆ Chelsea         ┆ false          ┆ Beats thrums   ┆ POSITIVE  ┆ beat thrum     │\n",
       "│ llum_frodo_bagg ┆ Martinez        ┆                ┆ with an        ┆           ┆ unbridled      │\n",
       "│ …               ┆                 ┆                ┆ unbridled…     ┆           ┆ energy ow…     │\n",
       "│ phenomenal_vali ┆ Mike Joseph     ┆ false          ┆ A sequel that  ┆ POSITIVE  ┆ sequel         │\n",
       "│ ant_michael_cor ┆                 ┆                ┆ - narratively  ┆           ┆ narratively    │\n",
       "│ …               ┆                 ┆                ┆ an…            ┆           ┆ visually se…   │\n",
       "└─────────────────┴─────────────────┴────────────────┴────────────────┴───────────┴────────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make changes as necessary\n",
    "# inside the map_elements, add  the parameters [pre_process(x, parameters_to_be_added)] and set it True/False if it differs from the defualt value\n",
    "df_train = df_train.with_columns(\n",
    "    pl.col(response_column).map_elements(lambda x: pre_process(x, remove_brackets=True)).alias(\"processed\")  #add inside the map_elements\n",
    ")\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### in this template, there are four text representation / vectorizer methods available \n",
    "#### #in the function run_pipeline, we shall make use of this, write the words inside [ ] for the methods you want to use\n",
    "#### 1. Bag of Words [BOW] \n",
    "#### 2. Term Frequency [tf]\n",
    "#### 3. TF -IDF    [tfidf]\n",
    "#### 4. Word Embedding using Word2Vec (you can use other packages with slight changes) [wv] \n",
    "         # Word Embedding uses defualt 300 values; this will take some time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### in this template, there are also three machine learning methods that can be used\n",
    "#### 1. Logistic Regression [logit]\n",
    "#### 2. Random forest (recommended) (rf)\n",
    "#### 3. XGBoosting  [XGB](word embedding and XGBoost may take long time to complete, combination of both is not recommended in local machine)\n",
    "\n",
    "#I will keep this repository updated, and I will add more methods in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Pipeline for Bow + Rf ---\n",
      "WARNING: Dropped 1586 rows due to missing values (None) in 'processed' or 'sentiment' columns. Original rows: 40689, Rows after dropping: 39103\n",
      "Labels encoded: Original -> ['NEGATIVE' 'POSITIVE'], Encoded -> [0 1]\n",
      "1. Vectorizing entire dataset (X)...\n",
      "   - Generating Bag-of-Words features...\n",
      "2. Splitting data into train/test...\n",
      "3. Training and predicting...\n",
      "   - Starting Random Forest training with GridSearchCV for hyperparameter tuning...\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# this is the example of how to use the function\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# you can change the vectorizer_name and model_name to the ones you want to use\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# for now we will use word embedding and logistic regression\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# run_pipeline function will return the dataframe with the vectorized text, vectorizer used  and the model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# it will also print the results of the model, including the accuracy and F1 score\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m dt\u001b[38;5;241m=\u001b[39m run_pipeline(\n\u001b[0;32m     10\u001b[0m     vectorizer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBOW\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# BOW, tf, tfidf, wv\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# logit, rf, XGB .#XGB takes long time, can not recommend using it on normal case\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf_train,\n\u001b[0;32m     13\u001b[0m     text_column_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# this is the column name of the text data, \u001b[39;00m\n\u001b[0;32m     14\u001b[0m     sentiment_column_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# this is the column name of the label data,\u001b[39;00m\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\meala\\Dropbox\\Data Work\\Text NLP\\tools\\pipeline.py:97\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[1;34m(vectorizer_name, model_name, df, text_column_name, sentiment_column_name)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Train + predict\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. Training and predicting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 97\u001b[0m y_pred, trained_model_object \u001b[38;5;241m=\u001b[39m train_and_predict_function(X_train, y_train, X_test)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4. Evaluating model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\meala\\Dropbox\\Data Work\\Text NLP\\MLAlgo\\rf.py:50\u001b[0m, in \u001b[0;36mtrain_and_predict\u001b[1;34m(X_train, y_train, X_test)\u001b[0m\n\u001b[0;32m     39\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     40\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mrf_model,\n\u001b[0;32m     41\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Print progress messages\u001b[39;00m\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Fit GridSearchCV to the training data\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# This will perform the hyperparameter search\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Get the best model found by GridSearchCV\u001b[39;00m\n\u001b[0;32m     53\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\meala\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\meala\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1046\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1047\u001b[0m     )\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1051\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\meala\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1605\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\meala\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    993\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    994\u001b[0m         )\n\u001b[0;32m    995\u001b[0m     )\n\u001b[1;32m--> 997\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    998\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    999\u001b[0m         clone(base_estimator),\n\u001b[0;32m   1000\u001b[0m         X,\n\u001b[0;32m   1001\u001b[0m         y,\n\u001b[0;32m   1002\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m   1003\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m   1004\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m   1005\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m   1006\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m   1007\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m   1008\u001b[0m     )\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m   1011\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m   1012\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m )\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\meala\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[0;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     75\u001b[0m     (\n\u001b[0;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config_and_warning_filters)\n",
      "File \u001b[1;32mc:\\Users\\meala\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\meala\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\meala\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\meala\\anaconda3\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\meala\\anaconda3\\Lib\\threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this is the example of how to use the function\n",
    "# you can change the vectorizer_name and model_name to the ones you want to use\n",
    "# for now we will use word embedding and logistic regression\n",
    "# write the name of your columns in the text_column_name and sentiment_column_name\n",
    "# the text_column_name is the column name of the text data, and sentiment_column_name is\n",
    "\n",
    "# run_pipeline function will return the dataframe with the vectorized text, vectorizer used  and the model\n",
    "# it will also print the results of the model, including the accuracy and F1 score\n",
    "dt= run_pipeline(\n",
    "    vectorizer_name=\"BOW\", # BOW, tf, tfidf, wv\n",
    "    model_name=\"rf\", # logit, rf, XGB .#XGB takes long time, can not recommend using it on normal case\n",
    "    df=df_train,\n",
    "    text_column_name=\"processed\",  # this is the column name of the text data, \n",
    "    sentiment_column_name = \"sentiment\"  # this is the column name of the label data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model_object', 'vectorizer_name', 'vectorizer_object', 'label_encoder', 'y_test', 'y_pred', 'accuracy', 'report'])\n",
      "Vectorizer used:  BOW\n",
      "Model used:  LogisticRegression(max_iter=500, random_state=42, solver='liblinear')\n",
      "Accuracy:  0.775092699143332\n"
     ]
    }
   ],
   "source": [
    "## the dt is a dictionary that contains the results of the model, including the accuracy and F1 score\n",
    "print(dt.keys())\n",
    "# you can access the results using the keys of the dictionary\n",
    "print(\"Vectorizer used: \", dt[\"vectorizer_name\"])\n",
    "print(\"Model used: \", dt[\"model_object\"])\n",
    "print(\"Accuracy: \", dt[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Dataset for prediction\n",
    "You can use the same format as the training dataset, but ensure that it contains the \"Response\" column for text data. The \"Sentiment\" column is optional for prediction datasets, as it will be generated by the model.\n",
    "Make sure the dataset is saved in the \"New Data\" folder and is in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55315, 4)\n",
      "(13828, 4)\n"
     ]
    }
   ],
   "source": [
    "new_data = pl.read_csv(\"New Data/test.csv\",encoding='ISO-8859-1') #keep your file here\n",
    "print(new_data.shape)\n",
    "new_data= new_data.sample(fraction=0.25, shuffle=True, seed=42)\n",
    "print(new_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meala\\AppData\\Local\\Temp\\ipykernel_17248\\3427585582.py:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  new_data = new_data.with_columns(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>movieid</th><th>reviewerName</th><th>isTopCritic</th><th>reviewText</th><th>processed</th></tr><tr><td>str</td><td>str</td><td>bool</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;frodo_baggins_rocky_balboa_she…</td><td>&quot;Toni Vaughn&quot;</td><td>false</td><td>&quot;The result is an unsettling ta…</td><td>&quot;result unsettling tale human p…</td></tr><tr><td>&quot;stardust_john_mcclane&quot;</td><td>&quot;Carol Jennings&quot;</td><td>false</td><td>&quot;Think twice about getting invo…</td><td>&quot;think twice getting involved w…</td></tr><tr><td>&quot;hermione_granger_sherlock_holm…</td><td>&quot;Tara Huang&quot;</td><td>true</td><td>&quot;A film that&#x27;s so bloody wonder…</td><td>&quot;film bloody wonderful meaning …</td></tr><tr><td>&quot;astonish_valiant&quot;</td><td>&quot;Shelley Murillo&quot;</td><td>false</td><td>&quot;...a decent setup that&#x27;s emplo…</td><td>&quot;decent setup employed progress…</td></tr><tr><td>&quot;indiana_jones_dazzling_dorothy…</td><td>&quot;Daniel Bond&quot;</td><td>false</td><td>&quot;Inspiring? Not to me. Lamentab…</td><td>&quot;inspiring lamentably bland exe…</td></tr><tr><td>&quot;glimmer_hannibal_lecter_frodo_…</td><td>&quot;Mrs. Nicole Fleming&quot;</td><td>false</td><td>&quot;This perfectly executed piece …</td><td>&quot;perfectly executed piece movie…</td></tr><tr><td>&quot;brave_anakin_skywalker&quot;</td><td>&quot;Melissa Harrington&quot;</td><td>false</td><td>&quot;The Richard Curtis script has …</td><td>&quot;richard curtis script rooney m…</td></tr><tr><td>&quot;katniss_everdeen_superman&quot;</td><td>&quot;Mckenzie Ortiz&quot;</td><td>false</td><td>&quot;A fun-filled afternoon of dino…</td><td>&quot;funfilled afternoon dinotastic…</td></tr><tr><td>&quot;secret_magic_john_wick_legend&quot;</td><td>&quot;Samantha Ware&quot;</td><td>false</td><td>&quot;I&#x27;m still not sure what this i…</td><td>&quot;still sure supposed save bunch…</td></tr><tr><td>&quot;gandalf_the_grey_magic_wandere…</td><td>&quot;Seth Downs&quot;</td><td>false</td><td>&quot;If the lizard ain&#x27;t broken, do…</td><td>&quot;lizard ai nt broken nt fix&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 5)\n",
       "┌─────────────────────┬────────────────────┬─────────────┬────────────────────┬────────────────────┐\n",
       "│ movieid             ┆ reviewerName       ┆ isTopCritic ┆ reviewText         ┆ processed          │\n",
       "│ ---                 ┆ ---                ┆ ---         ┆ ---                ┆ ---                │\n",
       "│ str                 ┆ str                ┆ bool        ┆ str                ┆ str                │\n",
       "╞═════════════════════╪════════════════════╪═════════════╪════════════════════╪════════════════════╡\n",
       "│ frodo_baggins_rocky ┆ Toni Vaughn        ┆ false       ┆ The result is an   ┆ result unsettling  │\n",
       "│ _balboa_she…        ┆                    ┆             ┆ unsettling ta…     ┆ tale human p…      │\n",
       "│ stardust_john_mccla ┆ Carol Jennings     ┆ false       ┆ Think twice about  ┆ think twice        │\n",
       "│ ne                  ┆                    ┆             ┆ getting invo…      ┆ getting involved   │\n",
       "│                     ┆                    ┆             ┆                    ┆ w…                 │\n",
       "│ hermione_granger_sh ┆ Tara Huang         ┆ true        ┆ A film that's so   ┆ film bloody        │\n",
       "│ erlock_holm…        ┆                    ┆             ┆ bloody wonder…     ┆ wonderful meaning  │\n",
       "│                     ┆                    ┆             ┆                    ┆ …                  │\n",
       "│ astonish_valiant    ┆ Shelley Murillo    ┆ false       ┆ ...a decent setup  ┆ decent setup       │\n",
       "│                     ┆                    ┆             ┆ that's emplo…      ┆ employed progress… │\n",
       "│ indiana_jones_dazzl ┆ Daniel Bond        ┆ false       ┆ Inspiring? Not to  ┆ inspiring          │\n",
       "│ ing_dorothy…        ┆                    ┆             ┆ me. Lamentab…      ┆ lamentably bland   │\n",
       "│                     ┆                    ┆             ┆                    ┆ exe…               │\n",
       "│ glimmer_hannibal_le ┆ Mrs. Nicole        ┆ false       ┆ This perfectly     ┆ perfectly executed │\n",
       "│ cter_frodo_…        ┆ Fleming            ┆             ┆ executed piece …   ┆ piece movie…       │\n",
       "│ brave_anakin_skywal ┆ Melissa Harrington ┆ false       ┆ The Richard Curtis ┆ richard curtis     │\n",
       "│ ker                 ┆                    ┆             ┆ script has …       ┆ script rooney m…   │\n",
       "│ katniss_everdeen_su ┆ Mckenzie Ortiz     ┆ false       ┆ A fun-filled       ┆ funfilled          │\n",
       "│ perman              ┆                    ┆             ┆ afternoon of dino… ┆ afternoon          │\n",
       "│                     ┆                    ┆             ┆                    ┆ dinotastic…        │\n",
       "│ secret_magic_john_w ┆ Samantha Ware      ┆ false       ┆ I'm still not sure ┆ still sure         │\n",
       "│ ick_legend          ┆                    ┆             ┆ what this i…       ┆ supposed save      │\n",
       "│                     ┆                    ┆             ┆                    ┆ bunch…             │\n",
       "│ gandalf_the_grey_ma ┆ Seth Downs         ┆ false       ┆ If the lizard      ┆ lizard ai nt       │\n",
       "│ gic_wandere…        ┆                    ┆             ┆ ain't broken, do…  ┆ broken nt fix      │\n",
       "└─────────────────────┴────────────────────┴─────────────┴────────────────────┴────────────────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = new_data.with_columns(\n",
    "    pl.col(response_column).map_elements(lambda x: pre_process(x, remove_brackets=True)).alias(\"processed\")  #add inside the map_elements\n",
    ")\n",
    "new_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (13_181, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>movieid</th><th>reviewerName</th><th>isTopCritic</th><th>reviewText</th><th>processed</th><th>sentiment_predictions</th></tr><tr><td>str</td><td>str</td><td>bool</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;frodo_baggins_rocky_balboa_she…</td><td>&quot;Toni Vaughn&quot;</td><td>false</td><td>&quot;The result is an unsettling ta…</td><td>&quot;result unsettling tale human p…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;stardust_john_mcclane&quot;</td><td>&quot;Carol Jennings&quot;</td><td>false</td><td>&quot;Think twice about getting invo…</td><td>&quot;think twice getting involved w…</td><td>&quot;NEGATIVE&quot;</td></tr><tr><td>&quot;hermione_granger_sherlock_holm…</td><td>&quot;Tara Huang&quot;</td><td>true</td><td>&quot;A film that&#x27;s so bloody wonder…</td><td>&quot;film bloody wonderful meaning …</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;astonish_valiant&quot;</td><td>&quot;Shelley Murillo&quot;</td><td>false</td><td>&quot;...a decent setup that&#x27;s emplo…</td><td>&quot;decent setup employed progress…</td><td>&quot;NEGATIVE&quot;</td></tr><tr><td>&quot;indiana_jones_dazzling_dorothy…</td><td>&quot;Daniel Bond&quot;</td><td>false</td><td>&quot;Inspiring? Not to me. Lamentab…</td><td>&quot;inspiring lamentably bland exe…</td><td>&quot;NEGATIVE&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;vivid_intrigue_celestial&quot;</td><td>&quot;Luke Reyes&quot;</td><td>false</td><td>&quot;This is the film&#x27;s ultimate me…</td><td>&quot;film ultimate message nt think…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;mystery_lost_james_t._kirk_sta…</td><td>&quot;Kathy Wade&quot;</td><td>false</td><td>&quot;It&#x27;s the best Sondheim adaptat…</td><td>&quot;best sondheim adaptation sayin…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;hiccup_terminator_myriad_capta…</td><td>&quot;Troy Watson&quot;</td><td>false</td><td>&quot;The South Australian actor her…</td><td>&quot;south australian actor deliver…</td><td>&quot;NEGATIVE&quot;</td></tr><tr><td>&quot;rocky_balboa_dazzling_phantom&quot;</td><td>&quot;Wanda Peterson&quot;</td><td>true</td><td>&quot;Too slight to register as anyt…</td><td>&quot;slight register anything stunt…</td><td>&quot;NEGATIVE&quot;</td></tr><tr><td>&quot;kingdom_adventure_vito_corleon…</td><td>&quot;Gina Powers&quot;</td><td>true</td><td>&quot;Nods to Jack Pierce&#x27;s iconic, …</td><td>&quot;nod jack pierce iconic karloff…</td><td>&quot;POSITIVE&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (13_181, 6)\n",
       "┌────────────────┬────────────────┬─────────────┬────────────────┬────────────────┬────────────────┐\n",
       "│ movieid        ┆ reviewerName   ┆ isTopCritic ┆ reviewText     ┆ processed      ┆ sentiment_pred │\n",
       "│ ---            ┆ ---            ┆ ---         ┆ ---            ┆ ---            ┆ ictions        │\n",
       "│ str            ┆ str            ┆ bool        ┆ str            ┆ str            ┆ ---            │\n",
       "│                ┆                ┆             ┆                ┆                ┆ str            │\n",
       "╞════════════════╪════════════════╪═════════════╪════════════════╪════════════════╪════════════════╡\n",
       "│ frodo_baggins_ ┆ Toni Vaughn    ┆ false       ┆ The result is  ┆ result         ┆ POSITIVE       │\n",
       "│ rocky_balboa_s ┆                ┆             ┆ an unsettling  ┆ unsettling     ┆                │\n",
       "│ he…            ┆                ┆             ┆ ta…            ┆ tale human p…  ┆                │\n",
       "│ stardust_john_ ┆ Carol Jennings ┆ false       ┆ Think twice    ┆ think twice    ┆ NEGATIVE       │\n",
       "│ mcclane        ┆                ┆             ┆ about getting  ┆ getting        ┆                │\n",
       "│                ┆                ┆             ┆ invo…          ┆ involved w…    ┆                │\n",
       "│ hermione_grang ┆ Tara Huang     ┆ true        ┆ A film that's  ┆ film bloody    ┆ POSITIVE       │\n",
       "│ er_sherlock_ho ┆                ┆             ┆ so bloody      ┆ wonderful      ┆                │\n",
       "│ lm…            ┆                ┆             ┆ wonder…        ┆ meaning …      ┆                │\n",
       "│ astonish_valia ┆ Shelley        ┆ false       ┆ ...a decent    ┆ decent setup   ┆ NEGATIVE       │\n",
       "│ nt             ┆ Murillo        ┆             ┆ setup that's   ┆ employed       ┆                │\n",
       "│                ┆                ┆             ┆ emplo…         ┆ progress…      ┆                │\n",
       "│ indiana_jones_ ┆ Daniel Bond    ┆ false       ┆ Inspiring? Not ┆ inspiring      ┆ NEGATIVE       │\n",
       "│ dazzling_dorot ┆                ┆             ┆ to me.         ┆ lamentably     ┆                │\n",
       "│ hy…            ┆                ┆             ┆ Lamentab…      ┆ bland exe…     ┆                │\n",
       "│ …              ┆ …              ┆ …           ┆ …              ┆ …              ┆ …              │\n",
       "│ vivid_intrigue ┆ Luke Reyes     ┆ false       ┆ This is the    ┆ film ultimate  ┆ POSITIVE       │\n",
       "│ _celestial     ┆                ┆             ┆ film's         ┆ message nt     ┆                │\n",
       "│                ┆                ┆             ┆ ultimate me…   ┆ think…         ┆                │\n",
       "│ mystery_lost_j ┆ Kathy Wade     ┆ false       ┆ It's the best  ┆ best sondheim  ┆ POSITIVE       │\n",
       "│ ames_t._kirk_s ┆                ┆             ┆ Sondheim       ┆ adaptation     ┆                │\n",
       "│ ta…            ┆                ┆             ┆ adaptat…       ┆ sayin…         ┆                │\n",
       "│ hiccup_termina ┆ Troy Watson    ┆ false       ┆ The South      ┆ south          ┆ NEGATIVE       │\n",
       "│ tor_myriad_cap ┆                ┆             ┆ Australian     ┆ australian     ┆                │\n",
       "│ ta…            ┆                ┆             ┆ actor her…     ┆ actor deliver… ┆                │\n",
       "│ rocky_balboa_d ┆ Wanda Peterson ┆ true        ┆ Too slight to  ┆ slight         ┆ NEGATIVE       │\n",
       "│ azzling_phanto ┆                ┆             ┆ register as    ┆ register       ┆                │\n",
       "│ m              ┆                ┆             ┆ anyt…          ┆ anything       ┆                │\n",
       "│                ┆                ┆             ┆                ┆ stunt…         ┆                │\n",
       "│ kingdom_advent ┆ Gina Powers    ┆ true        ┆ Nods to Jack   ┆ nod jack       ┆ POSITIVE       │\n",
       "│ ure_vito_corle ┆                ┆             ┆ Pierce's       ┆ pierce iconic  ┆                │\n",
       "│ on…            ┆                ┆             ┆ iconic, …      ┆ karloff…       ┆                │\n",
       "└────────────────┴────────────────┴─────────────┴────────────────┴────────────────┴────────────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_predictions(\n",
    "    new_data=new_data,\n",
    "    text_column_name=\"processed\",\n",
    "    vectorizer=dt[\"vectorizer_object\"],\n",
    "    best_model=dt[\"model_object\"],\n",
    "    label_encoder=dt[\"label_encoder\"],\n",
    "    prediction_column_name=\"sentiment_predictions\"  # Optional custom name\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
