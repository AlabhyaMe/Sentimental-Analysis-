{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#locally install the package\n",
    "! pip install ..\\dist\\quick_sentiments-0.1.6-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6602e",
   "metadata": {},
   "source": [
    "## Main Fucntion pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c5fe71",
   "metadata": {},
   "source": [
    "`run_pipeline` is the second main function to run the whole pipeline. When the data is cleaned with `pre_process`, it can be passed to `run_pipeline`. This function will take df, the vectorization method, the machine learning methods to be used, the columns that was pre-processed and the columns that will be used for the target variable. It also takes parameter tunng for the machine learning methods. By defualt, the parameter tuning is set to False. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "017eecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "df = pl.read_csv(\"training_data/train.csv\", encoding=\"utf-8\")\n",
    "df = df[1:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cf724bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quick_sentiments import pre_process\n",
    "from quick_sentiments import run_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa1e133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_column = \"reviewText\" \n",
    "sentiment_column = \"sentiment\"\n",
    "df = df.with_columns(\n",
    "    pl.col(response_column).map_elements(lambda x: pre_process(x, remove_brackets=True)).alias(\"processed\")  #add inside the map_elements\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0909ab9",
   "metadata": {},
   "source": [
    "This is how the function is called:\n",
    "\n",
    "def run_pipeline(\n",
    "\n",
    "    vectorizer_name: str,  <- name the vectorization method (\"BOW\", \"tf\", \"tfidf\", \"wv\")\n",
    "\n",
    "    model_name: str,     <- name the machine learning method (\"logit\", \"rf\", \"XGB\")\n",
    "\n",
    "    df: Union[pl.DataFrame, pd.DataFrame],  <- the dataframe that contains the pre-processed text data and the target variable\n",
    "\n",
    "    text_column_name: str,  <- the column name of the pre-processed text data, the function needs to know,\n",
    "\n",
    "    sentiment_column_name: str,  <- the column name of the target variable, the function needs to know,\n",
    "\n",
    "    perform_tuning: bool = False <- whether to perform hyperparameter tuning or not, default is False\n",
    "    \n",
    "):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681338d9",
   "metadata": {},
   "source": [
    "The run_pipeline can be used after the data is pre-processed with `pre_process`. It will return a dictionary with the results of the model training and evaluation. The dictionary will contain the following keys:\n",
    "- `model`: the trained model, this will be used for prediction later\n",
    "- `vectorizer_name`: the name of the vectorizer used for the text data, it will be used for prediction later\n",
    "- `vectorizer_object`: the fitted vectorizer object, this will be used for prediction later\n",
    "- `label_encoder`: we will need this to decode the labels back to their original form\n",
    "- `y_test`: the true labels of the test data\n",
    "- `y_pred`: the predicted labels of the test data\n",
    "- `accuracy`: the accuracy of the model on the test data\n",
    "- `report`: the classification report of the model on the test data\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"model_object\": trained_model_object,\n",
    "        \"vectorizer_name\": vectorizer_name,\n",
    "        \"vectorizer_object\": fitted_vectorizer_object,\n",
    "        \"label_encoder\": label_encoder,\n",
    "        \"y_test\": y_test,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"report\": classification_report(y_test, y_pred, output_dict=True, target_names=label_encoder.classes_)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dbcfcb",
   "metadata": {},
   "source": [
    "#### Running BOW and Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c35a58",
   "metadata": {},
   "source": [
    "No tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3c7f8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Pipeline for Bow + Logit ---\n",
      "WARNING: Dropped 14 rows due to missing values (None) in 'processed' or 'sentiment' columns. Original rows: 499, Rows after dropping: 485\n",
      "Labels encoded: Original -> ['NEGATIVE' 'POSITIVE'], Encoded -> [0 1]\n",
      "1. Vectorizing entire dataset (X)...\n",
      "   - Generating Bag-of-Words features...\n",
      "2. Splitting data into train/test...\n",
      "3. Training and predicting...\n",
      "   - Training Logistic Regression with default parameters (no hyperparameter tuning)...\n",
      "   - Model trained with default parameters.\n",
      "Best model parameters: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "4. Evaluating model...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.50      0.18      0.27        33\n",
      "    POSITIVE       0.68      0.91      0.78        64\n",
      "\n",
      "    accuracy                           0.66        97\n",
      "   macro avg       0.59      0.54      0.52        97\n",
      "weighted avg       0.62      0.66      0.60        97\n",
      "\n",
      "True labels distribution: Counter({np.int64(1): 64, np.int64(0): 33})\n",
      "Predicted labels distribution: Counter({np.int64(1): 85, np.int64(0): 12})\n"
     ]
    }
   ],
   "source": [
    "dt= run_pipeline(\n",
    "    vectorizer_name=\"BOW\", # BOW, tf, tfidf, wv\n",
    "    model_name=\"logit\", # logit, rf, XGB .#XGB takes long time, can not recommend using it on normal case\n",
    "    df=df,\n",
    "    text_column_name=\"processed\",  # this is the column name of the text data, \n",
    "    sentiment_column_name = \"sentiment\",\n",
    "    perform_tuning = False# make this true if you want to perform hyperparameter tuning, it will take longer time and \n",
    "                            # may run out of memory if the dataset is large,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe5c9f",
   "metadata": {},
   "source": [
    "With parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eccbe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Pipeline for Bow + Logit ---\n",
      "WARNING: Dropped 14 rows due to missing values (None) in 'processed' or 'sentiment' columns. Original rows: 499, Rows after dropping: 485\n",
      "Labels encoded: Original -> ['NEGATIVE' 'POSITIVE'], Encoded -> [0 1]\n",
      "1. Vectorizing entire dataset (X)...\n",
      "   - Generating Bag-of-Words features...\n",
      "2. Splitting data into train/test...\n",
      "3. Training and predicting...\n",
      "   - Starting Logistic Regression training with GridSearchCV for hyperparameter tuning...\n",
      "   - Using default parameter grid for tuning: {'solver': ['liblinear', 'lbfgs'], 'C': [0.1, 1.0, 10.0], 'class_weight': [None, 'balanced'], 'max_iter': [500, 1000]}\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "\n",
      "   - Best Hyperparameters found:\n",
      "{'C': 10.0, 'class_weight': 'balanced', 'max_iter': 500, 'solver': 'liblinear'}\n",
      "   - Best Cross-Validation Score (F1-weighted): 0.6243\n",
      "Best model parameters: {'C': 10.0, 'class_weight': 'balanced', 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 500, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "4. Evaluating model...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.48      0.30      0.37        33\n",
      "    POSITIVE       0.70      0.83      0.76        64\n",
      "\n",
      "    accuracy                           0.65        97\n",
      "   macro avg       0.59      0.57      0.56        97\n",
      "weighted avg       0.62      0.65      0.63        97\n",
      "\n",
      "True labels distribution: Counter({np.int64(1): 64, np.int64(0): 33})\n",
      "Predicted labels distribution: Counter({np.int64(1): 76, np.int64(0): 21})\n"
     ]
    }
   ],
   "source": [
    "dt= run_pipeline(\n",
    "    vectorizer_name=\"BOW\", # BOW, tf, tfidf, wv\n",
    "    model_name=\"logit\", # logit, rf, XGB .#XGB takes long time, can not recommend using it on normal case\n",
    "    df=df,\n",
    "    text_column_name=\"processed\",  # this is the column name of the text data, \n",
    "    sentiment_column_name = \"sentiment\",\n",
    "    perform_tuning = True# make this true if you want to perform hyperparameter tuning, it will take longer time and \n",
    "                            # may run out of memory if the dataset is large,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea3e7c",
   "metadata": {},
   "source": [
    "Using TF-IDF and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "934b0961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Pipeline for Tfidf + Rf ---\n",
      "WARNING: Dropped 14 rows due to missing values (None) in 'processed' or 'sentiment' columns. Original rows: 499, Rows after dropping: 485\n",
      "Labels encoded: Original -> ['NEGATIVE' 'POSITIVE'], Encoded -> [0 1]\n",
      "1. Vectorizing entire dataset (X)...\n",
      "   - Generating TF-IDF features...\n",
      "2. Splitting data into train/test...\n",
      "3. Training and predicting...\n",
      "   - Training Random Forest with default parameters (no hyperparameter tuning)...\n",
      "   - Model trained with default parameters.\n",
      "4. Evaluating model...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.50      0.03      0.06        33\n",
      "    POSITIVE       0.66      0.98      0.79        64\n",
      "\n",
      "    accuracy                           0.66        97\n",
      "   macro avg       0.58      0.51      0.42        97\n",
      "weighted avg       0.61      0.66      0.54        97\n",
      "\n",
      "True labels distribution: Counter({np.int64(1): 64, np.int64(0): 33})\n",
      "Predicted labels distribution: Counter({np.int64(1): 95, np.int64(0): 2})\n"
     ]
    }
   ],
   "source": [
    "dt= run_pipeline(\n",
    "    vectorizer_name=\"tfidf\", \n",
    "    model_name=\"rf\", \n",
    "    df=df,\n",
    "    text_column_name=\"processed\",   \n",
    "    sentiment_column_name = \"sentiment\",\n",
    "    perform_tuning = False\n",
    "                            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f8b30ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Pipeline for Tfidf + Rf ---\n",
      "WARNING: Dropped 14 rows due to missing values (None) in 'processed' or 'sentiment' columns. Original rows: 499, Rows after dropping: 485\n",
      "Labels encoded: Original -> ['NEGATIVE' 'POSITIVE'], Encoded -> [0 1]\n",
      "1. Vectorizing entire dataset (X)...\n",
      "   - Generating TF-IDF features...\n",
      "2. Splitting data into train/test...\n",
      "3. Training and predicting...\n",
      "   - Starting Random Forest training with GridSearchCV for hyperparameter tuning...\n",
      "   - Using default parameter grid for tuning: {'n_estimators': [100, 200, 300], 'max_depth': [10, 20, None], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2], 'class_weight': [None, 'balanced']}\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "\n",
      "   - Best Hyperparameters found:\n",
      "{'class_weight': 'balanced', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "   - Best Cross-Validation Score (F1-weighted): 0.6001\n",
      "4. Evaluating model...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.36      0.55      0.43        33\n",
      "    POSITIVE       0.68      0.50      0.58        64\n",
      "\n",
      "    accuracy                           0.52        97\n",
      "   macro avg       0.52      0.52      0.51        97\n",
      "weighted avg       0.57      0.52      0.53        97\n",
      "\n",
      "True labels distribution: Counter({np.int64(1): 64, np.int64(0): 33})\n",
      "Predicted labels distribution: Counter({np.int64(0): 50, np.int64(1): 47})\n"
     ]
    }
   ],
   "source": [
    "dt= run_pipeline(\n",
    "    vectorizer_name=\"tfidf\", \n",
    "    model_name=\"rf\", \n",
    "    df=df,\n",
    "    text_column_name=\"processed\",   \n",
    "    sentiment_column_name = \"sentiment\",\n",
    "    perform_tuning = True\n",
    "                            \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_quick_sentiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
