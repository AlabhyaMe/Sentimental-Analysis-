{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfb99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install .\\dist\\quick_sentiments-0.1.6-py3-none-any.whl\n",
    "\n",
    "# the package can be seperately installed from PyPI however the dependencies are not up to date on PyPI\n",
    "# pip install -i https://test.pypi.org/simple/ quick-sentiments \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92698656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data 'punkt' already present.\n",
      "NLTK data 'stopwords' already present.\n",
      "Downloading NLTK data: wordnet...\n",
      "NLTK data 'wordnet' downloaded.\n",
      "Downloading NLTK data: omw-1.4...\n",
      "NLTK data 'omw-1.4' downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\meala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\meala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#load the packages\n",
    "import polars as pl\n",
    "#make sure that this is the main file\n",
    "import sys\n",
    "import os\n",
    "project_root = os.getcwd()\n",
    "sys.path.insert(0, project_root)\n",
    "# here I have three python script I built to pre_process the data and running the pipeline\n",
    "# you can find the code in the tools/preprocess.py file\n",
    "# you can find  the code in the tools/pipeline.py file\n",
    "# the pre_process function is used to clean the text data, there are various options available, please check the tools/preprocess.py file for details\n",
    "# the run_pipeline function is used to run the sentimental analysis pipeline, it takes the training data and the vectorizer and machine learning methods as input, and returns the results\n",
    "from quick_sentiments import pre_process\n",
    "#this function will run the sentimental analysis in the training data and return the results\n",
    "from quick_sentiments import run_pipeline\n",
    "# this function will run the sentimental analysis in the new data and return the predictions\n",
    "from quick_sentiments import make_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8273408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"training_data/train.csv\" #giive path to the training data\n",
    "df_train = pl.read_csv(path1, has_header=True, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "389c704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use the pre_process function to clean the text data\n",
    "response_column = \"reviewText\" # feel free to change the column name to your text column name\n",
    "sentiment_column = \"sentiment\" # feel free to change the column name to your label column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e1bb9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(fraction=0.1, shuffle=True, seed=42) \n",
    "df_train = df_train.with_columns(\n",
    "    pl.col(response_column).map_elements(lambda x: pre_process(x)).alias(\"processed\")  #add inside the map_elements\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f707fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Pipeline for Bow + Logit ---\n",
      "WARNING: Dropped 651 rows due to missing values (None) in 'processed' or 'sentiment' columns. Original rows: 16275, Rows after dropping: 15624\n",
      "Labels encoded: Original -> ['NEGATIVE' 'POSITIVE'], Encoded -> [0 1]\n",
      "1. Vectorizing entire dataset (X)...\n",
      "   - Generating Bag-of-Words features...\n",
      "2. Splitting data into train/test...\n",
      "3. Training and predicting...\n",
      "   - Training Logistic Regression with default parameters (no hyperparameter tuning)...\n",
      "   - Model trained with default parameters.\n",
      "Best model parameters: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "4. Evaluating model...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.66      0.52      0.59      1044\n",
      "    POSITIVE       0.78      0.87      0.82      2081\n",
      "\n",
      "    accuracy                           0.75      3125\n",
      "   macro avg       0.72      0.69      0.70      3125\n",
      "weighted avg       0.74      0.75      0.74      3125\n",
      "\n",
      "True labels distribution: Counter({np.int64(1): 2081, np.int64(0): 1044})\n",
      "Predicted labels distribution: Counter({np.int64(1): 2299, np.int64(0): 826})\n"
     ]
    }
   ],
   "source": [
    "dt= run_pipeline(\n",
    "    vectorizer_name=\"BOW\", # BOW, tf, tfidf, wv\n",
    "    model_name=\"logit\", # logit, rf, XGB .#XGB takes long time, can not recommend using it on normal case\n",
    "    df=df_train,\n",
    "    text_column_name=\"processed\",  # this is the column name of the text data, \n",
    "    sentiment_column_name = \"sentiment\",\n",
    "    perform_tuning = False # make this true if you want to perform hyperparameter tuning, it will take longer time and \n",
    "                            # may run out of memory if the dataset is large,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e70b8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = \"new_data/test.csv\" #give path to the test data\n",
    "df_test = pl.read_csv(path2, has_header=True, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd696ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = df_test.with_columns(\n",
    "    pl.col(response_column).map_elements(lambda x: pre_process(x, remove_brackets=True)).alias(\"processed\")  #add inside the map_elements\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f085ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_prediction= make_predictions(\n",
    "    new_data=new_data,\n",
    "    text_column_name=\"processed\",\n",
    "    vectorizer=dt[\"vectorizer_object\"],\n",
    "    best_model=dt[\"model_object\"],\n",
    "    label_encoder=dt[\"label_encoder\"],\n",
    "    prediction_column_name=\"sentiment_predictions\"  # Optional custom name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the predictions to a csv file\n",
    "# missing values in the text data will be removed\n",
    "sentiments_prediction.write_csv(\"demo/new_data/sentiments_prediction.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quick_sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
